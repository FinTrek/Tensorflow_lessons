{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolutional Layers**\n",
    "==\n",
    "\n",
    "So to do image recognition we need to create a convolutional network!\n",
    "\n",
    "This takes photo data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "color_channels = 1\n",
    "image_height = 28\n",
    "image_width  = 28\n",
    "\n",
    "\n",
    "class CNN:\n",
    "    def __init__(self, image_height, image_width, channels, num_classes):\n",
    "        #this input layer holds the image data as is. in the tensor placeholder \n",
    "        self.input_layer = tf.placeholder(dtype=tf.float32, shape=[None, image_height, image_width, channels], name=\"inputs\")\n",
    "        print(self.input_layer.shape)\n",
    "        \n",
    "        #this convolutional layer takes the input data and grabs the kernel_size and breaks\n",
    "        #that size off and passes it through an activation function that will test similarities to the rest of the image,\n",
    "        conv_layer_1 = tf.layers.conv2d(self.input_layer, filters=32, kernel_size=[2, 2], padding=\"same\", activation=tf.nn.relu)\n",
    "        print(conv_layer_1.shape)\n",
    "         \n",
    "        #the pooling layer removes the precision of the image data\n",
    "        pooling_layer_1 = tf.layers.max_pooling2d(conv_layer_1, pool_size=[2,2], strides=2)\n",
    "        print(pooling_layer_1.shape)\n",
    "         \n",
    "        #runs a second convolutional layer to take chunk up a second time\n",
    "        conv_layer_2 = tf.layers.conv2d(pooling_layer_1, filters=64, kernel_size=[2, 2], padding=\"same\", activation=tf.nn.relu)\n",
    "        print(conv_layer_2.shape)\n",
    "        \n",
    "        #the pooling layer does the same  shrinkning our# data sets\n",
    "        pooling_layer_2 = tf.layers.max_pooling2d(conv_layer_2, pool_size=[2, 2], strides=2)\n",
    "        print(pooling_layer_2.shape)\n",
    "        \n",
    "        #flatten the data\n",
    "        flattened_pooling = tf.layers.flatten(pooling_layer_2)\n",
    "        print(\"Flattened: \" + str(flattened_pooling.shape) )\n",
    "        \n",
    "        #develops a neural network containing 1024\n",
    "        dense_layer = tf.layers.dense(flattened_pooling, 1024, activation=tf.nn.relu)\n",
    "        print(\"Dense: \" + str(dense_layer.shape))\n",
    "        dropout = tf.layers.dropout(dense_layer, rate=0.4, training=True)\n",
    "        print(\"Dropout: \" + str(dropout.shape))\n",
    "        outputs = tf.layers.dense(dropout, num_classes)\n",
    "        print(\"Outputs: \" + str(outputs.shape))\n",
    "         \n",
    "        self.choice = tf.argmax(outputs, axis=1)\n",
    "        self.probability = tf.nn.softmax(outputs)\n",
    "         \n",
    "        self.labels = tf.placeholder(dtype=tf.float32, name=\"labels\")\n",
    "        self.accuracy, self.accuracy_op = tf.metrics.accuracy(self.labels, self.choice)\n",
    "         \n",
    "        one_hot_labels = tf.one_hot(indices=tf.cast(self.labels, dtype=tf.int32), depth=num_classes)     \n",
    "        self.loss = tf.losses.softmax_cross_entropy(onehot_labels=one_hot_labels, logits=outputs)\n",
    "         \n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-2)\n",
    "        self.train_operation = optimizer.minimize(self.loss, global_step=tf.train.get_global_step())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/fashion/train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion/train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "data_images = input_data.read_data_sets('data/fashion/')\n",
    "#data_labels = input_data.read_data_sets('data/fashion/train_labels')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 ... 4 5 6]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#eval_data = input_data.read_data_sets('data/fashion/eval_images')\n",
    "#eval_labels = input_data.read_data_sets('data/fashion/eval_labels')\n",
    "#print(data)\n",
    "\n",
    "eval_im = data_images.test.images\n",
    "eval_lab = np.asarray(data_images.test.labels, dtype=np.int32)\n",
    "print(eval_lab)\n",
    "e = eval_im[0].reshape(28, 28)\n",
    "plt.imshow(e)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 3 4 ... 5 6 8]\n",
      "(55000, 784)\n",
      "(55000,)\n",
      "(?, 28, 28, 1)\n",
      "(?, 28, 28, 32)\n",
      "(?, 14, 14, 32)\n",
      "(?, 14, 14, 64)\n",
      "(?, 7, 7, 64)\n",
      "Flattened: (?, 3136)\n",
      "Dense: (?, 1024)\n",
      "Dropout: (?, 1024)\n",
      "Outputs: (?, 10)\n",
      "Accuracy: 0.0625\n",
      "Accuracy: 0.1171875\n",
      "Accuracy: 0.109375\n",
      "Accuracy: 0.10546875\n",
      "Accuracy: 0.109375\n",
      "Accuracy: 0.104166664\n",
      "Accuracy: 0.09598214\n",
      "Accuracy: 0.1015625\n",
      "Accuracy: 0.10243055\n",
      "Accuracy: 0.1109375\n",
      "Accuracy: 0.12215909\n",
      "Accuracy: 0.12760417\n",
      "Accuracy: 0.12740384\n",
      "Accuracy: 0.13169643\n",
      "Accuracy: 0.13229166\n",
      "Accuracy: 0.13769531\n",
      "Accuracy: 0.1360294\n",
      "Accuracy: 0.13541667\n",
      "Accuracy: 0.13898027\n",
      "Accuracy: 0.1390625\n",
      "Accuracy: 0.13988096\n",
      "Accuracy: 0.13849431\n",
      "Accuracy: 0.14266305\n",
      "Accuracy: 0.14257812\n",
      "Accuracy: 0.144375\n",
      "Accuracy: 0.1454327\n",
      "Accuracy: 0.14699075\n",
      "Accuracy: 0.14955357\n",
      "Accuracy: 0.15086207\n",
      "Accuracy: 0.153125\n",
      "Accuracy: 0.1547379\n",
      "Accuracy: 0.15478516\n",
      "Accuracy: 0.15482955\n",
      "Accuracy: 0.15487133\n",
      "Accuracy: 0.15892857\n",
      "Accuracy: 0.16059028\n",
      "Accuracy: 0.15962838\n",
      "Accuracy: 0.15995066\n",
      "Accuracy: 0.15985577\n",
      "Accuracy: 0.16289063\n",
      "Accuracy: 0.16425306\n",
      "Accuracy: 0.16555059\n",
      "Accuracy: 0.17005815\n",
      "Accuracy: 0.171875\n",
      "Accuracy: 0.17430556\n",
      "Accuracy: 0.17527173\n",
      "Accuracy: 0.17785904\n",
      "Accuracy: 0.17936198\n",
      "Accuracy: 0.18367347\n",
      "Accuracy: 0.185\n",
      "Accuracy: 0.18627451\n",
      "Accuracy: 0.18900241\n",
      "Accuracy: 0.1898585\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-36ea769912b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mbatch_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mbatch_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_operation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_labels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m#print(\"Loss: \" + sess.run(cnn.loss, feed_dict={cnn.input_layer:batch_inputs}))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tenv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tenv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tenv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tenv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tenv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "training_steps = 2000\n",
    "batch_size = 64\n",
    "\n",
    "image_height = 28\n",
    "image_width  = 28\n",
    "\n",
    "color_channels = 1\n",
    "\n",
    "train_data = data_images.train.images\n",
    "train_labels = np.asarray(data_images.train.labels, dtype=np.int32) \n",
    "\n",
    "print(train_labels)\n",
    "#preps the labels for training\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "\n",
    "\n",
    "cnn = CNN(image_height, image_width, color_channels, 10)\n",
    "\n",
    "#prep the evaluation data\n",
    "\n",
    "\n",
    "#eval_labels = np.asarray(data.train.labels, dtype=np.int32)\n",
    "\n",
    "train_data = train_data.reshape(-1, image_height, image_width, color_channels)\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels))\n",
    "dataset = dataset.shuffle(buffer_size = train_labels.shape[0])\n",
    "dataset =dataset.batch(batch_size)\n",
    "dataset = dataset.repeat()\n",
    "dataset_iterator = dataset.make_initializable_iterator()\n",
    "next_element = dataset_iterator.get_next()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(dataset_iterator.initializer)\n",
    "    for step in range(training_steps):\n",
    "        current_batch = sess.run(next_element)\n",
    "        \n",
    "        batch_inputs = current_batch[0]\n",
    "        batch_labels = current_batch[1]\n",
    "        sess.run((cnn.train_operation, cnn.accuracy_op), feed_dict={cnn.input_layer:batch_inputs, cnn.labels:batch_labels})\n",
    "        print(\"Accuracy: \" + str(sess.run(cnn.accuracy)))\n",
    "        #print(\"Loss: \" + sess.run(cnn.loss, feed_dict={cnn.input_layer:batch_inputs}))\n",
    "    save_path = saver.save(sess, \"/tmp/model.ckpt\")\n",
    "    #print(\"Model saved in path: %s\" % save_path)\n",
    "    #sess.run(cnn.accuracy_op, feed_dict={cnn.input_layer:eval_im, cnn.labels:eval_lab})\n",
    "    #print(sess.run(cnn.accuracy))\n",
    "    #for image, label in zip(eval_data, eval_labels):\n",
    "    #    sess.run(cnn.accuracy_op, feed_dict={cnn.input_layer:[image], cnn.labels:label})\n",
    "    #print(sess.run(cnn.accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28, 1)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "eval_im = eval_im.reshape(-1, image_height, image_width, color_channels)\n",
    "\n",
    "print(eval_im.shape)\n",
    "print(eval_lab.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "My Guess was: 7\n",
      "[[1.2168059e-06 5.3418973e-03 8.5414993e-03 3.3964089e-01 3.4243043e-04\n",
      "  3.6075743e-04 8.0172667e-06 9.9624344e+01 6.0292183e-05 2.1356588e-02]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#evaluate_set = tf.data.Dataset.from_tensor_slices(eval_data)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    #saver.restore(sess, \"/tmp/model.ckpt\")\n",
    "    #print(sess.run(cnn.accuracy_op, feed_dict={cnn.input_layer:eval_im, cnn.labels:eval_lab}))\n",
    "    #print(sess.run(cnn.choice))\n",
    "\n",
    "    img = eval_im[122]\n",
    "    img = img.reshape(28, 28)\n",
    "    plt.imshow(img)\n",
    "    guess = sess.run(cnn.choice, feed_dict={cnn.input_layer:[eval_im[122]]})\n",
    "    guess_name = str(guess[0])\n",
    "    print(\"My Guess was: \" + guess_name)\n",
    "    print(sess.run(cnn.probability, feed_dict={cnn.input_layer:[eval_im[122]]})*100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = data.train.images[1]\n",
    "#eval_label = data.train.label(np.asarray(dat))\n",
    "img = np.array(eval_data, dtype='float')\n",
    "pix = img.reshape(28, 28)\n",
    "plt.imshow(pix)\n",
    "\n",
    "eval_labels = np.asarray(data.train.labels, dtype=np.int32)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
