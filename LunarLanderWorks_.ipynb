{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import utils as np_utils\n",
    "from keras.layers import Dense, Activation, Input, BatchNormalization, Conv2D\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras.preprocessing import image\n",
    "from PIL import Image\n",
    "from gym import envs\n",
    "print(envs.registry.all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims = [16, 16]):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.__build_network(input_dim, output_dim, hidden_dims)\n",
    "        self.__build_train_fn()\n",
    "        \n",
    "    def __build_network(self, input_dim, output_dim, hidden_dims):\n",
    "        #creates base network\n",
    "        self.X = Input(shape = (8,))\n",
    "        net = self.X\n",
    "        net = keras.layers.Conv2D(kernel_size = (10, 10), filters = 8, strides = 3)(net)\n",
    "        net = keras.layers.Conv2D(kernel_size = (8, 8), filters = 8, s\n",
    "                                  rides = 3)(net)\n",
    "        net = keras.layers.Flatten()(net)\n",
    "        net = Dense(16)(net)\n",
    "        net = Activation(\"relu\")(net)\n",
    "        net = BatchNormalization()(net)\n",
    "        net = Dense(output_dim)(net)\n",
    "        net = Activation(\"softmax\")(net)\n",
    "        self.model = Model(inputs = self.X, outputs = net)\n",
    "        self.model.summary()\n",
    "    def __build_train_fn(self):\n",
    "        action_prob_placeholder     = self.model.output #placeholder to hold the probabilities for each action\n",
    "        action_onehot_placeholder   = K.placeholder(shape = (None, self.output_dim), name = 'action_onehot')\n",
    "        discount_reward_placeholder = K.placeholder(shape = (None, ), name = \"discounted_reward\")\n",
    "        \n",
    "        action_prob = K.sum(K.log(action_prob_placeholder) * action_onehot_placeholder)\n",
    "        loss = -action_prob * discount_reward_placeholder\n",
    "        \n",
    "        loss = K.mean(loss)\n",
    "        adam = optimizers.Adam(lr = 0.001)\n",
    "        \n",
    "        updates = adam.get_updates(params = self.model.trainable_weights, \n",
    "                                  loss = loss)\n",
    "        \n",
    "        self.train_fn = K.function(inputs = [self.model.input,\n",
    "                                            action_onehot_placeholder,\n",
    "                                            discount_reward_placeholder],\n",
    "                                  outputs = [],\n",
    "                                  updates = updates)\n",
    "    def get_action(self, state):\n",
    "        shape = state.shape\n",
    "            \n",
    "        action_prob = self.model.predict(state)\n",
    "        return np.random.choice(np.arange(self.output_dim), p = action_prob)\n",
    "    def fit(self, S, A, R):\n",
    "        action_onehot = np_utils.to_categorical(A, num_classes = self.output_dim)\n",
    "        discount_reward = compute_discounted_R(R)\n",
    "        self.train_fn([S, action_onehot, discount_reward])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_discounted_R(R, discount_rate = .99):\n",
    "    discounted_r = np.zeros_like(R, dtype=np.float32)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(len(R))):\n",
    "        running_add = running_add * discount_rate + R[t]\n",
    "        discounted_r[t] = running_add\n",
    "    discounted_r -= (discounted_r.mean()/discounted_r.std())\n",
    "    return discounted_r\n",
    "\n",
    "def run_episode(env, agent):\n",
    "    done = False\n",
    "    S = []\n",
    "    A = []\n",
    "    R = []\n",
    "    s = env.reset()\n",
    "    s = image.array_to_img(s)\n",
    "    s = s.resize((100, 100), Image.ANTIALIAS).convert('L')\n",
    "    s = image.img_to_array(s).reshape(-1, 100, 100, 1)\n",
    "    #print(s.shape)\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        env.render()\n",
    "        a = agent.get_action(s)\n",
    "        s2, r, done, info = env.step(a)\n",
    "        s2 = image.array_to_img(s2)\n",
    "        s2 = s2.resize((100, 100), Image.ANTIALIAS).convert('L')\n",
    "        s2 = image.img_to_array(s2).reshape(-1, 100, 100, 1)\n",
    "        #print(s2.shape)\n",
    "        total_reward += r\n",
    "        \n",
    "        S.append(s)\n",
    "        A.append(a)\n",
    "        R.append(r)\n",
    "        \n",
    "        s = s2\n",
    "        \n",
    "        if done:\n",
    "            S = np.array(S)\n",
    "            A = np.array(A)\n",
    "            R = np.array(R)\n",
    "            agent.fit(S, A, R)\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = env.observation_space.shape\n",
    "print(input_shape)\n",
    "output_shape = env.action_space.n\n",
    "print(output_shape)\n",
    "agent = Agent(input_shape, output_shape, [16, 16])\n",
    "\n",
    "for episode in range(2000):\n",
    "    reward = run_episode(env, agent)\n",
    "    print(episode, reward)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras-rl --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0809 15:14:23.353299 140509737899840 deprecation_wrapper.py:119] From /home/idstudent/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0809 15:14:23.371836 140509737899840 deprecation_wrapper.py:119] From /home/idstudent/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0809 15:14:23.388050 140509737899840 deprecation_wrapper.py:119] From /home/idstudent/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 68        \n",
      "=================================================================\n",
      "Total params: 756\n",
      "Trainable params: 756\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "its working\n",
      "Dimension 1 in both shapes must be equal, but are 4 and 5. Shapes are [16,4] and [16,5]. for 'Assign_6' (op: 'Assign') with input shapes: [16,4], [16,5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0809 15:14:23.826774 140509737899840 deprecation_wrapper.py:119] From /home/idstudent/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0809 15:14:23.827909 140509737899840 deprecation_wrapper.py:119] From /home/idstudent/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0809 15:14:24.090398 140509737899840 deprecation_wrapper.py:119] From /home/idstudent/.local/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "    1/10000 [..............................] - ETA: 1:51:04 - reward: -0.9378"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/idstudent/.local/lib/python3.7/site-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   21/10000 [..............................] - ETA: 25:05 - reward: -0.9578"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/idstudent/.local/lib/python3.7/site-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 117s 12ms/step - reward: -0.6426\n",
      "44 episodes - episode_reward: -146.948 [-1007.840, 52.927] - loss: 10.950 - mean_absolute_error: 22.159 - mean_q: 1.275\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 112s 11ms/step - reward: -0.1852\n",
      "25 episodes - episode_reward: -75.722 [-195.495, 48.410] - loss: 7.995 - mean_absolute_error: 34.963 - mean_q: 29.005\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 111s 11ms/step - reward: -0.0426\n",
      "14 episodes - episode_reward: -30.813 [-148.949, 113.459] - loss: 8.359 - mean_absolute_error: 37.478 - mean_q: 43.043\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 113s 11ms/step - reward: -0.0210\n",
      "14 episodes - episode_reward: -20.314 [-310.028, 115.515] - loss: 8.525 - mean_absolute_error: 38.462 - mean_q: 45.922\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 118s 12ms/step - reward: -0.1511\n",
      "15 episodes - episode_reward: -99.566 [-672.687, 131.334] - loss: 8.861 - mean_absolute_error: 36.080 - mean_q: 43.368\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 117s 12ms/step - reward: -0.0248\n",
      "14 episodes - episode_reward: -14.331 [-331.254, 146.318] - loss: 7.967 - mean_absolute_error: 33.562 - mean_q: 42.400\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 116s 12ms/step - reward: -0.0515\n",
      "11 episodes - episode_reward: -42.692 [-235.650, 131.371] - loss: 6.811 - mean_absolute_error: 31.385 - mean_q: 40.152\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 117s 12ms/step - reward: -0.0720\n",
      "15 episodes - episode_reward: -48.828 [-354.120, 141.864] - loss: 7.012 - mean_absolute_error: 32.553 - mean_q: 42.185\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 116s 12ms/step - reward: -0.0039\n",
      "13 episodes - episode_reward: -3.782 [-267.153, 148.735] - loss: 7.621 - mean_absolute_error: 32.454 - mean_q: 41.426\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 106s 11ms/step - reward: 0.0550\n",
      "11 episodes - episode_reward: 56.833 [-90.064, 148.670] - loss: 7.312 - mean_absolute_error: 30.705 - mean_q: 38.502\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 104s 10ms/step - reward: 0.0664\n",
      "11 episodes - episode_reward: 55.279 [-176.161, 140.148] - loss: 5.337 - mean_absolute_error: 30.037 - mean_q: 38.114\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      " 6358/10000 [==================>...........] - ETA: 39s - reward: -0.3262"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "\n",
    "ENV_NAME = 'LunarLander-v2'\n",
    "\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# Next, we build a very simple model regardless of the dueling architecture\n",
    "# if you enable dueling network in DQN , DQN will build a dueling network base on your model automatically\n",
    "# Also, you can build a dueling network by yourself and turn off the dueling network in DQN.\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions, activation='linear'))\n",
    "print(model.summary())\n",
    "try:\n",
    "    print(\"its working\")\n",
    "    model.load_weights('duel_dqn_{}_weights.h5f'.format(ENV_NAME))\n",
    "    print(\"it worked\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "policy = BoltzmannQPolicy()\n",
    "# enable the dueling network\n",
    "# you can specify the dueling_type to one of {'avg','max','naive'}\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "               enable_dueling_network=True, dueling_type='avg', target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "dqn.fit(env, nb_steps=5000000, visualize=True, verbose=1)\n",
    "\n",
    "# After training is done, we save the final weights.\n",
    "dqn.save_weights('duel_dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n",
    "\n",
    "\n",
    "\n",
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "dqn.test(env, nb_episodes=10, visualize=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
